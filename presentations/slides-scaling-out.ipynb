{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Scaling out: `xarray` & `dask`\n",
    "<br>\n",
    "\n",
    "<img src=\"https://github.com/pydata/xarray/raw/master/doc/_static/dataset-diagram.png\" alt=\"xarray Logo\" style=\"height: 150px;\">\n",
    "<img src=\"http://dask.readthedocs.io/en/latest/_images/dask_horizontal.svg\" \n",
    "     width=\"30%\" \n",
    "     align=right\n",
    "     alt=\"Dask logo\">\n",
    "     \n",
    "### *You create a DASK array by specifying chunks when reading in data from a netcdf file in x-array*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parallel computing with Dask\n",
    "Dask divides arrays into many small pieces, called chunks, each of which is presumed to be small enough to fit into memory.\n",
    "\n",
    "![Dask Arrays](http://dask.pydata.org/en/latest/_images/dask-array-black-text.svg)\n",
    "\n",
    "_source: [Dask Array Documentation](http://dask.pydata.org/en/latest/array-overview.html)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spinning up a cluster\n",
    "\n",
    "### Step 1: Start a `dask` cluster\n",
    "\n",
    "```python\n",
    "from dask_jobqueue import PBSCluster\n",
    "cluster = PBSCluster(cores=1, memory='10GB', processes=1, queue='share', \n",
    "                     walltime='01:00:00')\n",
    "cluster.scale(4) # Ask for 4 workers\n",
    "```\n",
    "*^ This is a Cheyenne-specific command bc PBS*\n",
    "\n",
    "### Step 2: Connect a client to it\n",
    "\n",
    "```python\n",
    "from dask.distributed import Client\n",
    "client = Client(cluster) # Connect this local process to remote workers\n",
    "client\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Notes on terminology (it's inconsistent!)\n",
    "\n",
    "The arguements to `PBSCluster` determine the *job* configuration:\n",
    "- `processes`: the number of *workers* per job;\n",
    "- `cores`: the number of cores per jobs, shared among the workers.\n",
    "\n",
    "**Multiple single-threaded workers per job:**\n",
    "\n",
    "```python\n",
    "n_nodes = 2\n",
    "cluster = PBSCluster(cores=18, processes=18, queue='regular')\n",
    "cluster.scale(18 * n_nodes) # Ask for 18 x n_nodes workers\n",
    "```\n",
    "*^ need to request the same number of cores & processes when creating many matplotlib plots*\n",
    "\n",
    "**Multi-threaded workers:**\n",
    "\n",
    "```python\n",
    "n_nodes = 2\n",
    "cluster = PBSCluster(cores=36, processes=9, queue='regular')\n",
    "cluster.scale(9 * n_nodes) # Ask for 9 x n_nodes workers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Distributed Clusters (http://distributed.dask.org/)\n",
    "\n",
    "Dask can be deployed on distributed infrastructure, such as a an HPC system or a cloud computing system.\n",
    "\n",
    "- `LocalCluster` - Creates a `Cluster` that can be executed locally. Each `Cluster` includes a `Scheduler` and `Worker`s. \n",
    "- `Client` - Connects to and drives computation on a distributed `Cluster`\n",
    "\n",
    "### Dask Jobqueue (http://jobqueue.dask.org/)\n",
    "\n",
    "- `PBSCluster` - Cheyenne\n",
    "- `SlurmCluster` - DAV\n",
    "- `LSFCluster`\n",
    "- etc.\n",
    "\n",
    "### Dask Kubernetes (http://kubernetes.dask.org/)\n",
    "\n",
    "- `KubeCluster` - Cloud systems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NCAR deployment details\n",
    "\n",
    "*You cannot use a DAV cluster from Cheyenne or vice versa.*\n",
    "\n",
    "### ncar-jobqueue enables interoperable notebooks:  \n",
    "```python\n",
    "from ncar_jobqueue import NCARCluster\n",
    "from distributed import Client\n",
    "cluster = NCARCluster(cores=1, processes=1, memory='10GB')\n",
    "cluster.scale(4)\n",
    "client = Client(cluster)\n",
    "```\n",
    "*^ In case you don't want to specify which one to use, NCAR has an \"NCARCluster\" command*\n",
    "- calls `PBSCluster` on Cheyenne\n",
    "- calls `SLURMCluster` on DAV\n",
    "\n",
    "\n",
    "### Omitted arguments use default settings:\n",
    "`~/.config/dask/jobqueue.yaml`  \n",
    "*^ Should become familiar with this file because one has to change their project number*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Connecting to the dashboard  \n",
    "\n",
    "![](./img/client-report.png)\n",
    "\n",
    "* To get from ^ to below, replace what looks like an IP address with the web address that pops up on your jupyter lab or jupyter notebooks\n",
    "\n",
    "![](./img/dask-labextension.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Understanding the dashboard\n",
    "\n",
    "Tour of the dashboard.\n",
    "\n",
    "*Can do this on your laptop because not requesting multiple nodes*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Chunking and performance\n",
    "\n",
    "The chunks parameter has critical performance implications when using Dask arrays. \n",
    "- Chunks too small: slow due to fixed overhead and queuing operations;\n",
    "- Chunks too big: wasted computational resource...blown memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lazy execution\n",
    "\n",
    "![](img/lazy-compute.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dask Delayed\n",
    "\n",
    "Imagine a function:\n",
    "```python\n",
    "def my_function(x):\n",
    "   # do computation, make a plot, etc. \n",
    "```    \n",
    "\n",
    "Use `dask.delayed` to make `my_function` lazy and apply to large number (`N`) of inputs:\n",
    "```python\n",
    "import dask\n",
    "\n",
    "results = []\n",
    "for i in range(N):\n",
    "    result_i = dask.delayed(my_function)(x[i])\n",
    "    results.append(results_i)\n",
    "\n",
    "# trigger computation of the results\n",
    "results = dask.persist(*results)\n",
    "```\n",
    "\n",
    "*\"x\" above should not be a dask array. If it is, the dask array already parallelizes.  \n",
    "Advantage is if you have a very slow for-loop (as above), then using dask will make this process way faster.   \n",
    "An example of a good use of this is to make a GIF or movie with plots*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hand-on examples\n",
    "\n",
    "```bash \n",
    "git clone https://github.com/ncar-hackathons/hands-on-examples\n",
    "cd hands-on-examples/scientific-computing\n",
    "```\n",
    "\n",
    "Notebooks:\n",
    "- xarray.ipynb\n",
    "- dask.ipynb\n",
    "- cesm-le/cesm-le-seaice-example.ipynb\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analysis]",
   "language": "python",
   "name": "conda-env-analysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
